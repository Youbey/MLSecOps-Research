\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{pdflscape}
\usepackage{geometry}
\usepackage{caption}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Taxonomy of Attacks in Federated Learning}
\author{Groupe 2 - MLSecOps}
\date{November 2025}

\begin{document}
\maketitle

\begin{abstract}
Federated Learning (FL) allows decentralized training of machine learning models without centralizing sensitive data. However, it remains vulnerable to multiple types of attacks. This section presents a taxonomy of known attacks, adapted from Liu et al. (2023) and extended with recent literature.
\end{abstract}

\section{Taxonomy of Attacks in Federated Learning}
\subsection{Overview}
Figure~\ref{fig:fl-taxonomy} provides an overview of the main attack surfaces in FL, grouped by layer: data, model, communication, and aggregation.

\begin{figure}
\centering
\includegraphics[width=\linewidth, angle=90]{cartographie.png}
\caption{Taxonomy of Attacks and Defenses in Federated Learning}
\label{fig:fl-taxonomy}
\end{figure}

\begin{landscape}
\begin{table}[ht]
\centering
\small
\caption{Comprehensive Mapping of Federated Learning Attacks}
\begin{tabularx}{1.2\textwidth}{l l X X}
\toprule
\textbf{Category} & \textbf{Subtype} & \textbf{Description} & \textbf{Key References} \\
\midrule
\textbf{Poisoning Attacks} & Data Poisoning & Manipulation of local datasets to bias global model updates & \cite{bagdasaryan2020backdoor,fang2020local,blanchard2017machine} \\
& Model Poisoning & Direct modification of gradients or model weights to introduce hidden behavior or degradation & \cite{bhagoji2019analyzing,baruch2019little} \\
\midrule
\textbf{Backdoor Attacks} & Trigger-based Backdoors & Introduction of hidden malicious patterns (triggers) that cause misclassification when activated & \cite{bagdasaryan2020backdoor} \\
\midrule
\textbf{Inference Attacks} & Membership Inference & Determining whether specific data points were used in training & \cite{nasr2019comprehensive,melis2019exploiting} \\
& Property Inference & Inferring sensitive attributes of users’ data from gradients or model parameters & \cite{melis2019exploiting} \\
& Gradient Inversion & Reconstructing training samples from shared gradients & \cite{zhu2019deep,geiping2020inverting} \\
\midrule
\textbf{Communication Attacks} & Man-in-the-Middle (MITM) & Intercepting or modifying updates in transit between clients and server & \cite{lyu2020threats} \\
& Sybil Attacks & A single adversary simulates multiple fake clients to bias aggregation & \cite{fung2018mitigating} \\
\midrule
\textbf{Free-rider Attacks} & Model Theft & Participants submit fake updates but still benefit from global model & \cite{wang2021attack} \\
\midrule
\textbf{Model Replacement} & Model Overwrite & Substituting the global model with a maliciously crafted one & \cite{bagdasaryan2020backdoor} \\
\bottomrule
\end{tabularx}
\end{table}
\end{landscape}

\subsection{Representative Works}
Table~\ref{tab:fl-attacks} summarizes key representative works, their publication venues, and their classification in the CORE database.

\begin{landscape}
\begin{table}
\centering
\footnotesize
\begin{tabular}{|p{3cm}|p{5cm}|p{3.5cm}|p{2cm}|p{6cm}|}
\hline
\textbf{Attack Type} & \textbf{Representative Paper} & \textbf{Venue} & \textbf{CORE Rank} & \textbf{Description} \\ \hline
Data Poisoning & Bagdasaryan et al., 2020 & AISTATS & A & Demonstrates model-replacement and backdoor attacks targeting the global model aggregation in FL. \\ \hline
Gradient Leakage / Inference & Zhu et al., 2019 & NeurIPS & A* & Shows that private training data can be reconstructed from shared gradients. \\ \hline
Property / Membership Inference & Melis et al., 2019 & IEEE S\&P & A* & Reveals feature leakage through model updates during collaborative training. \\ \hline
Privacy Analysis & Nasr et al., 2019 & IEEE S\&P & A* & Formalizes white-box inference attacks against centralized and federated models. \\ \hline
Byzantine / Robustness & Blanchard et al., 2017 & NeurIPS & A* & Introduces Byzantine-tolerant gradient aggregation methods for adversarial clients. \\ \hline
Sybil / Collusion Attacks & Fung et al., 2020 & RAID & A & Demonstrates Sybil-based poisoning and proposes FoolsGold defense. \\ \hline
Distributed Backdoors & Xie et al., 2020 & ICLR & A* & Introduces DBA attacks — coordinated small-scale poisoning to evade detection. \\ \hline
GAN-based Reconstruction & Hitaj et al., 2017 & ACM CCS & A* & Early work using GANs to extract data from collaborative learning setups. \\ \hline
\end{tabular}
\caption{Mapping of Attacks in Federated Learning: representative works and venues.}
\label{tab:fl-attacks}
\end{table}
\end{landscape}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
