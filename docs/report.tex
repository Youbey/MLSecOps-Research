\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage{float}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{pdflscape}
\usepackage{geometry}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{float}



\title{State of the Art: Design of an MLSecOps Pipeline that Anticipates Attacks - test}
\author{
Ayoub Bendraou \and
Aya Fsahi \and
Nazih Ouchta \and
Salim Ghoudane
}
\date{November 2025}



\begin{document}


\maketitle

\begin{abstract}
TODO 


\end{abstract}
\section{What is Federated Learning?}
\textbf{Federated Learning (FL)} is formally defined and described across the literature as:
\begin{itemize}

\item A machine learning setting where \textbf{many clients} (e.g., mobile devices or whole organizations) \textbf{collaboratively train a model} under the orchestration of a central server (e.g., service provider), while \textbf{keeping the training data decentralized} \cite{ref16}. 


\item A learning paradigm introduced in 2016 by McMahan et al. where the learning task is solved by a loose federation of participating devices (clients) coordinated by a central server \cite{ref16}. 


\item A mechanism that embodies the principles of \textbf{focused collection and data minimization} \cite{ref16}. Client raw data is stored locally and is explicitly not exchanged or transferred \cite{ref16}. Instead, \textbf{focused updates intended for immediate aggregation} are used to achieve the learning objective \cite{ref16}. 

\item A model that mitigates many of the \textbf{systemic privacy risks and costs} resulting from traditional, centralized Machine Learning (ML) and data science approaches \cite{ref16}. 


\item A \textbf{privacy-preserving decentralized approach}, which keeps raw data on devices and involves local ML training while eliminating data communication overhead \cite{ref13}. A federation of the learned and shared models is then performed on a central server to aggregate and share the built knowledge among participants \cite{ref13}. 


\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{4-lifecycle.png}
    \caption{the various actors in a federated learning system \cite{ref16}}
    \label{fig:placeholder}
\end{figure}






\subsection{Motives and Need for Federated Learning}
The necessity for FL arose from the convergence of several technology trends and inherent failures within previous machine learning (ML) architectures, particularly regarding data privacy and scalability.
\subsubsection{The Data Revolution and AI's Needs}
The first motive is the \textbf{unprecedented availability of data} generated by connected devices, such as smartphones, Internet-of-Things (IoT) devices, and wearable medical devices
. These modern mobile devices possess powerful sensors (cameras, microphones, GPS) and are frequently carried, giving them access to a \textbf{wealth of data}
.
Since ML models are \textbf{data hunger}
 and Deep Learning (DL) applications have experienced vigorous growth, models trained on this rich, real-world data hold the promise of \textbf{greatly improving usability} by powering more intelligent applications, such as improving speech recognition and text entry
.
\subsubsection{Centralized Learning Failures and Privacy Mandates}
The conventional ML architecture relies on a \textbf{cloud-centric architecture} where data is continuously streamed, centrally stored, and processed
. This centralized model faced severe limitations:
\begin{itemize} \item \textbf{High Risk of Disclosure:} The \textbf{sensitive nature of the data} means there are risks and responsibilities to storing it centrally
. Without serious privacy consideration, sensitive data is \textbf{highly exposed to disclosure, attacks, and cyber risks}. Breaches like Equifax, Marriott, and eBay demonstrated these risks in recent memory. \item \textbf{Regulatory Compliance Challenges:} Stringent regulations like the European Union’s \textbf{GDPR} and the \textbf{HIPAA} guidelines strictly limit data sharing and storage. These acts challenge centralized models, as patient personal indicators cannot be shared without consent, meaning centralized AI-driven health analytics models are challenged with the quality of shared data (often anonymized, yielding generic interpretability). \item \textbf{Cost and Latency:} Centralized practices engage \textbf{unacceptable latency and high cost}
. \end{itemize}
\subsubsection*{1.3. On-Device Learning Isolation}
While running ML models locally on devices (on-site ML) solves the data transfer problem, it creates a new drawback: the local models are limited to \textbf{each user’s experience} and do not \textbf{benefit from peers' data}
.
\paragraph{FL as the Solution:}
FL was developed to enable users to \textbf{collectively reap the benefits of shared models} trained from rich data, without the need to centrally store it
. This approach allows the decoupling of model training from the need for direct access to the raw training data
.

\subsection{Mechanism: How does Federated Learning work?}
The standard FL workflow is orchestrated by a central server and is often based on the \textbf{Federated Averaging (FedAvg) algorithm} \cite{ref16, ref13}.
\subsubsection*{Architecture and Workflow}
The FL life cycle consists of several continuous communication rounds until the global model reaches the desired accuracy \cite{ref13}.

\begin{enumerate} 

\item \textbf{Client Selection/Initiation:} The server generates or initializes a generic model \cite{ref13}. In each round, a subset of clients is sampled \cite{ref16}. Typical eligibility requirements include the device being \textbf{in charge, idle, and on an unmetered connection} (i.e., Wi-Fi) \cite{ref13}. Advanced protocols like \textbf{FedCS} may require clients to report information about their resources (upload and update time) to a Mobile-Edge Computing (MEC) server, which then determines the subset able to complete the FL steps within a deadline \cite{ref13}. 

\item \textbf{Broadcast and Download:} Selected clients download the current model parameters/weights and a training program (e.g., a TensorFlow graph) from the server \cite{ref16, 13}. The global updates might propagate to local nodes at different time instants depending on communication channel specifics \cite{ref15}. 



\item \textbf{Local Training:} Each device locally trains and optimizes the global model using its local data \cite{ref13}. This involves running SGD \cite{ref13}. 

\item \textbf{Upload:} The client sends its locally computed parameters/updates to the central server \cite{ref13}. 


\item \textbf{Aggregation:} The server collects and aggregates the updates. This process is often a \textbf{weighted average} based on client dataset size \cite{ref13, 15}. 

\item \textbf{Model Update:} The server uses the aggregated gradient to update the global model. This new shared model is distributed back to the clients, and the process repeats until convergence \cite{ref13, 15}. \end{enumerate} In this star network communication topology, a subset of selected devices performs local training on their \textbf{non-identically-distributed user data} and sends these local updates to the server \cite{ref14}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{3.training process.png}
    \caption{FL training process.}
    \label{fig:placeholder}
\end{figure}


\subsubsection{Alternative Learning Paradigms and Techniques} 

\begin{itemize} 

\item \textbf{Fully Decentralized / Peer-to-Peer Distributed Learning:} This approach removes the need for a central server coordinating the overall computation, with devices communicating directly with neighbors \cite{ref16}. 


\item \textbf{FL Algorithms for Aggregation:} \begin{itemize} 

\item \textbf{FedPer (Federated learning with personalization layer):} Clients send base layers (for representational learning) to the server but retain higher layers (decision specifics) locally \cite{ref15}. 

\item \textbf{FedMA (Federated learning with matched averaging):} Constructs a global model by averaging values of hidden layers (e.g., convolutional layers) \cite{ref15}. 

\item \textbf{FedDist (Federated Distance):} Computes the distance between neurons with similar features, suitable for sparse data \cite{ref15}. 

\item \textbf{One-shot FL:} The massive number of communication rounds is substituted with only \textbf{one round} where each device trains its model until completion, and models are aggregated using ensemble learning techniques \cite{ref13}. \end{itemize} 

\item \textbf{Data Partitioning:} 


\begin{itemize} \item \textbf{Horizontal FL (HFL):} Local clients have the \textbf{same feature sets} but in \textbf{different data spaces} \cite{ref15}. \item \textbf{Vertical FL (VFL):} Local clients have the \textbf{same sample space} but \textbf{different feature sets} \cite{ref15}. \item \textbf{Transfer Learning-based FL (TL-FL):} Extends VFL to include more local clients with different feature sets but the same sample space, extracting common features to a shared space, suitable for EHRs collected from heterogeneous sources \cite{ref15}. \end{itemize} \end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{data part.png}
    \caption{FL-HI aggregation conceptual overview: The left figure depicts the horizontal FL, centre figure depicts the vertical FL type, and the right
figure indicates the transfer learning scheme in FL \cite{ref15}}
    \label{fig:placeholder}
\end{figure}



\section{Technical overview}

\subsection{Federated Optimization Problem Formulation}
FL focuses on supervised machine learning (ML), where the goal is to find model parameters $\mathbf{w}$ (a vector) that minimize a global loss function $F(\mathbf{w})$ over a massive, partitioned dataset.

\paragraph{The Global Objective Function (Non-IID Setting)}
The overall optimization goal is to minimize the finite-sum objective function $F(\mathbf{w})$,
which is a weighted sum of the local loss functions $F_k(\mathbf{w})$ across $K$ clients:
\[
\min_{\mathbf{w}} F(\mathbf{w}), \quad \text{where  }   \textcolor{white}{-} F(\mathbf{w}) \stackrel{\text{def}}{=} \sum_{k=1}^{K} \frac{n_k}{n} F_k(\mathbf{w})
\]
Where:
\begin{itemize}
    \item $K$ is the total number of clients participating in the learning rounds.
    \item $n_k$ is the number of data samples on client $k$, where $n_k = |P_k|$ ($P_k$ is the partition of data assigned to client $k$).
    \item $n$ is the total number of samples across all clients, where $n = \sum_{k=1}^{K} n_k$.
    \item $F_k(\mathbf{w})$ is the \textbf{local objective function} (empirical risk) for the $k$th client, typically defined as $F_k(\mathbf{w}) = \frac{1}{n_k} \sum_{i \in P_k} f_i(\mathbf{w})$, where $f_i(\mathbf{w})$ is the loss function for the $i$th data sample $(x_i, y_i)$.
\end{itemize}

\subsection{The Federated Averaging (FedAvg) Algorithm}
FedAvg is the chosen practical algorithm for FL implementation, combining local Stochastic Gradient Descent (SGD) on each client with a server that performs iterative model averaging.
The algorithm is designed to use additional local computation to \textbf{reduce the number of communication rounds} by $10$--$100\times$ compared to synchronized SGD.

The algorithm uses three key parameters to control computation:
\begin{itemize}
    \item $C$: The fraction of clients selected to perform computation in each round.
    \item $E$: The number of local training passes (epochs) each selected client makes over its local dataset in one communication round.
    \item $B$: The local minibatch size used for client updates. ($B = \infty$ means the full local dataset is used as one minibatch).
\end{itemize}
The number of local updates per round for client $k$ is $u_k = E \frac{n_k}{B}$.

\paragraph{FedAvg Pseudocode (Adapted from Algorithm 1)}
\hrule
\vspace{0.1cm}
\noindent \textbf{Server Executes:}
\begin{enumerate}
    \item Initialize global model weights $\mathbf{w}_0$.
    \item \textbf{For each round $t = 1, 2, \ldots, T$ do:}
    \begin{itemize}
        \item $m \leftarrow \max(C \cdot K, 1)$ (Number of clients selected).
        \item $S_t \leftarrow$ (random set of $m$ clients).
        \item \textbf{For each client $k \in S_t$ in parallel do:} $\mathbf{w}_k^{t+1} \leftarrow \text{ClientUpdate}(k, \mathbf{w}_t)$
        \item \textbf{Aggregate (Weighted Average):} The new global model $\mathbf{w}_{t+1}$ is calculated as the weighted sum of the returned local models, where $\sum_{k=1}^{K} \frac{n_k}{n} = 1$.
        \[
        \mathbf{w}_{t+1} \leftarrow \sum_{k \in S_t} \frac{n_k}{n} \mathbf{w}_k^{t+1}
        \]
    \end{itemize}
\end{enumerate}
\vspace{0.1cm}
\noindent \hrule
\vspace{0.1cm}
\noindent \textbf{ClientUpdate($k$, $\mathbf{w}$):} (\textbf{Run on client $k$})
\begin{enumerate}
    \item $\mathcal{B} \leftarrow$ (split local dataset $P_k$ into batches of size $B$).
    \item \textbf{For each local epoch $i$ from 1 to $E$ do:}
    \begin{itemize}
        \item \textbf{For batch $b \in \mathcal{B}$ do:} $\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla \ell(\mathbf{w}; b)$
    \end{itemize}
    \item return $\mathbf{w}$ to server.
\end{enumerate}
\vspace{0.1cm}
\hrule

\subsubsection*{FedSGD as a Baseline}
FedAvg generalizes \textbf{Federated SGD (FedSGD)}, which is the simplest synchronous update scheme.
FedSGD corresponds exactly to FedAvg when $B = \infty$ (full local batch) and $E = 1$ (one local epoch).

\begin{itemize}
    \item In FedSGD, each client $k$ computes $\mathbf{g}_k = \nabla F_k(\mathbf{w}_t)$, the average gradient on its local data.
    \item The server aggregates the gradients and applies the update $\mathbf{w}_{t+1} \leftarrow \mathbf{w}_t - \eta \sum_{k=1}^{K} \frac{n_k}{n} \mathbf{g}_k$.
    \item This is mathematically equivalent to the model averaging: $\forall k, \mathbf{w}_k^{t+1} \leftarrow \mathbf{w}_t - \eta \mathbf{g}_k$ and then $\mathbf{w}_{t+1} \leftarrow \sum_{k=1}^{K} \frac{n_k}{n} \mathbf{w}_k^{t+1}$.
\end{itemize}


\section{Applications}
FL is deployed across massive consumer products (Cross-Device) and organizations with high confidentiality needs (Cross-Silo).
\subsubsection*{Consumer, Mobile, and Edge Devices (Cross-Device FL)} \begin{itemize} \item \textbf{Mobile Keyboards:} FL was first tested on Google's virtual keyboard, \textbf{Gboard} \cite{ref13}. It is used to improve \textbf{next-word prediction}, \textbf{query suggestion}, word completion, corrections \cite{ref13}, and \textbf{emoji prediction} \cite{ref13}. FL is also adapted to learn \textbf{out-of-vocabulary (OOV) words} \cite{ref13}. \item \textbf{Mobile Operating Systems:} Used for features on \textbf{Pixel phones} and in \textbf{Android Messages} \cite{ref16}. Apple uses cross-device FL in \textbf{iOS 13} for applications like the \textbf{QuickType keyboard} and the \textbf{vocal classifier for “Hey Siri”} \cite{ref16}. \item \textbf{General Mobile Apps:} Powers applications such as face detection and voice recognition \cite{ref14}. Explored for \textbf{hotword detection} \cite{ref16}. \item \textbf{IoT Systems:} Used to limit vulnerabilities in large-scale IoT systems \cite{ref13}. Use cases include \textbf{intrusion detection systems} based on anomaly detection \cite{ref13} and \textbf{Mobile Edge Computing (MEC)} optimization \cite{ref13}. \item \textbf{Electric Vehicles (EVs):} Analyzing driver behavior metrics (using LSTM) to predict battery failure \cite{ref13}. \end{itemize}
\subsubsection*{Organizational and Industrial (Cross-Silo FL)} \begin{itemize} \item \textbf{Healthcare Informatics (HI):} FL is a strong fit for HI, balancing patient privacy with ML by keeping patient data on-premise \cite{ref13}. Applications include: \begin{itemize} \item Predicting mortality and hospital stay time using distributed \textbf{Electronic Health Records (EHRs)} \cite{ref13}. \item Predicting hospitalizations for heart disease patients \cite{ref13}. \item \textbf{Medical image prediction} (e.g., brain tumor segmentation) \cite{ref13, 15}. \item Collaborative learning for multi-modal \textbf{COVID-19 diagnosis} with X-ray and Ultrasound imagery \cite{ref15}. \item Detecting \textbf{adverse events in mass-scale vaccination programs} \cite{ref15}. \item Analysis of biomedical data (e.g., subcordinal brain changes in neurological disease) \cite{ref13}. \item Drug discovery (e.g., MELLODDY project) \cite{ref15}. \item Training on \textbf{multi-institutional datasets} constrained by HIPAA/FERPA \cite{ref16}. \end{itemize} \item \textbf{Finance:} \textbf{Finance risk prediction for reinsurance} \cite{ref16}. Collaborative training for \textbf{fraud detection} models \cite{ref16}. Used by Webank and Swiss Re for high-precise financial analysis \cite{ref15}. \item \textbf{Recommender Systems:} Generating personalized recommendations using collaborative filtering \cite{ref13}. \item \textbf{Online Retailers:} Analyzing user click-stream data to enhance prediction of consumer's next browsing activities \cite{ref13}. \item \textbf{Model Types:} Applicable to models that operate over large inventories where clients interact with only a tiny fraction of items (sparse problems), such as \textbf{natural language models} or \textbf{content ranking models} using an embedding lookup table \cite{ref16}. \end{itemize}



\section{Advantages}
FL provides significant advantages over traditional centralized and on-device machine learning models, primarily related to privacy, cost, and system efficiency.
\begin{itemize} \item \textbf{Data Privacy and Security Guarantees:} FL is a \textbf{privacy-preserving decentralized approach} that keeps \textbf{raw data on-devices} and precludes direct access to it \cite{ref13}. The approach involves sharing focused model updates (e.g., gradients) instead of the raw data \cite{ref14}. Local training preserves the privacy, confidentiality, and integrity of patient data \cite{ref15}. \item \textbf{Compliance and Risk Mitigation:} FL can \textbf{mitigate many of the systemic privacy risks and costs} resulting from traditional centralized approaches \cite{ref16}. It effectively handles the tradeoff between model learning and regulatory compliance (e.g., HIPAA/GDPR) because raw data is not centralized \cite{ref15}. \item \textbf{Communication and Network Efficiency:} FL eliminates the data communication overhead associated with centralized systems \cite{ref13}. By pushing computation to the edge and requiring only small, iterative model updates, it reduces strain on the network \cite{ref14}. \item \textbf{Knowledge Sharing (vs. On-Device ML):} It overcomes the limitation of isolated on-device ML (where models don't benefit from peers' data) by aggregating local models to \textbf{share knowledge} among participants \cite{ref13}. \item \textbf{Accuracy and Model Diversity:} FL achieves \textbf{high precision and accuracy} by leveraging a large volume of data across many clients \cite{ref13}. It also enables training on \textbf{multi-institutional datasets} (e.g., medical, education) where centralization was constrained, potentially leading to improved model \textbf{fairness} and diversity \cite{ref16}. \item \textbf{Resource and Latency Reduction:} FL handles the challenge of expensive centralized training \cite{ref15}. It resolves the network latency problem as clients process data locally, eliminating the need to fetch data from a remote server \cite{ref13}. \end{itemize}





\vspace{0.5cm} \noindent \rule{\textwidth}{0.5pt} \vspace{0.2cm} \textbf{Paper Titles Cited:} \begin{enumerate} \item A Survey on Federated Learning: The Journey From Centralized to Distributed On-Site Learning and Beyond \item Federated Learning: Challenges, Methods, and Future Directions \item Adoption of Federated Learning for Healthcare Informatics: Emerging Applications and Future Directions \item Advances and Open Problems in Federated Learning \item Communication-Efficient Learning of Deep Networks from Decentralized Data \end{enumerate}


