
@article{bagdasaryan2020backdoor,
  title={Backdoor attacks on federated learning},
  author={Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  journal={Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2020}
}

@article{fang2020local,
  title={Local Model Poisoning Attacks to Byzantine-Robust Federated Learning},
  author={Fang, Meng and Cao, Xiaofei and Jia, Jiawei and Gong, Neil Zhenqiang},
  journal={USENIX Security Symposium},
  year={2020}
}

@article{blanchard2017machine,
  title={Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent},
  author={Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2017}
}

@article{bhagoji2019analyzing,
  title={Analyzing Federated Learning through an Adversarial Lens},
  author={Bhagoji, Arjun Nitin and Chakraborty, Supriyo and Mittal, Prateek and Calo, Seraphin},
  journal={Proceedings of the 36th International Conference on Machine Learning (ICML)},
  year={2019}
}

@article{baruch2019little,
  title={A Little is Enough: Circumventing Defenses for Distributed Learning},
  author={Baruch, Gilad and Baruch, Moran and Goldberg, Yoav},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{nasr2019comprehensive,
  title={Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning},
  author={Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  journal={IEEE Symposium on Security and Privacy (S\&P)},
  year={2019}
}

@article{melis2019exploiting,
  title={Exploiting Unintended Feature Leakage in Collaborative Learning},
  author={Melis, Luca and Song, Congzheng and Cristofaro, Emiliano De and Shmatikov, Vitaly},
  journal={IEEE Symposium on Security and Privacy (S\&P)},
  year={2019}
}

@article{zhu2019deep,
  title={Deep Leakage from Gradients},
  author={Zhu, Ligeng and Liu, Zhijian and Han, Song},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{geiping2020inverting,
  title={Inverting Gradients - How easy is it to break privacy in federated learning?},
  author={Geiping, Jonas and Bauermeister, Hartmut and Dröge, Heiko and Moeller, Michael},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@article{lyu2020threats,
  title={Threats to Federated Learning: A Survey},
  author={Lyu, Lingjuan and Yu, Han and Yang, Qiang},
  journal={ACM Computing Surveys},
  year={2020}
}

@article{fung2018mitigating,
  title={Mitigating Sybils in Federated Learning Poisoning},
  author={Fung, Clement and Yoon, Chris J and Beschastnikh, Ivan},
  journal={Proceedings of the 21st International Symposium on Research in Attacks, Intrusions and Defenses (RAID)},
  year={2018}
}
@article{wang2021attack,
  title={Attack of the Tails: Yes, You Really Can Backdoor Federated Learning},
  author={Wang, Yu and Hu, Yuan and Li, Guoming and Lin, Zhiqiang},
  journal={USENIX Security Symposium},
  year={2021}
}

@article{ref13,
  title={A Survey on Federated Learning: The Journey From Centralized to Distributed On-Site Learning and Beyond},
  author={Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aurélien and Bennis, Mehdi and Bhagoji, Arjun Nitin and others},
  journal={IEEE Transactions on Machine Learning and Knowledge Extraction},
  year={2021}
}

@article{ref14,
  title={Federated Learning: Challenges, Methods, and Future Directions},
  author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal={IEEE Signal Processing Magazine},
  year={2020}
}

@article{ref15,
  title={Adoption of Federated Learning for Healthcare Informatics: Emerging Applications and Future Directions},
  author={Rieke, Nicola and Hancox, Jonny and Li, Wenqi and Milletarì, Fausto and Roth, Holger R. and Albarqouni, Shadi and Bakas, Spyridon and others},
  journal={npj Digital Medicine},
  year={2020}
}

@article{ref16,
  title={Advances and Open Problems in Federated Learning},
  author={Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
  journal={Foundations and Trends in Machine Learning},
  year={2019}
}

@article{ref17,
  title={Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author={McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  journal={Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2017}
}


@article{liu2021fedmlhe,
  title={FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving Federated Learning System},
  author={Liu, Yu and Zhang, Jing and Zhang, Ying and Lin, Zhen and Huang, Jie},
  journal={IEEE Transactions on Information Forensics and Security},
  year={2021}
}

@article{wei2020differential,
  title={Differential Privacy Federated Learning: A Comprehensive Review},
  author={Wei, Kaixuan and Li, Jian and Ding, Mingsong and Ma, Shiqi and Li, Zhi and Yang, Qiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2020}
}

@article{zhang2022privacy,
  title={Research on Privacy Protection Technology in Federated Learning},
  author={Zhang, Chen and Wang, Rui and Li, Yu and Zhao, Bo},
  journal={IEEE Access},
  year={2022}
}

@article{rieke2020healthcare,
  title={Adoption of Federated Learning for Healthcare Informatics: Emerging Applications and Future Directions},
  author={Rieke, Nicola and Hancox, Jonny and Li, Wenqi and Milletarì, Fausto and Roth, Holger R. and Albarqouni, Shadi and Bakas, Spyridon and others},
  journal={npj Digital Medicine},
  year={2020}
}

@inproceedings{blanchard2017machine,
  author    = {P. Blanchard and E. M. El Mhamdi and R. Guerraoui and J. Stainer},
  title     = {Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2017},
  url       = {https://arxiv.org/abs/1703.02757}
}

@inproceedings{yin2018byzantine,
  author    = {D. Yin and Y. Chen and R. Kannan and P. Bartlett},
  title     = {Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates},
  booktitle = {Proc. Int. Conf. Machine Learning (ICML)},
  year      = {2018},
  url       = {https://arxiv.org/abs/1803.01498}
}

@inproceedings{elmhamdi2018hidden,
  author    = {E. M. El Mhamdi and R. Guerraoui and S. Rouault},
  title     = {The Hidden Vulnerability of Distributed Learning in Byzantium},
  booktitle = {Proc. Int. Conf. Machine Learning (ICML)},
  year      = {2018},
  url       = {https://arxiv.org/abs/1802.07927}
}

@inproceedings{cao2021fltrust,
  author    = {X. Cao and M. Fang and J. Liu and N. Z. Gong},
  title     = {FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping},
  booktitle = {Proc. Network and Distributed System Security Symposium (NDSS)},
  year      = {2021},
  url       = {https://arxiv.org/abs/2012.13995}
}

@inproceedings{fang2020local,
  author    = {M. Fang and X. Cao and J. Jia and N. Z. Gong},
  title     = {Local Model Poisoning Attacks to Byzantine-Robust Federated Learning},
  booktitle = {Proc. USENIX Security Symp.},
  year      = {2020},
  url       = {https://www.usenix.org/conference/usenixsecurity20/presentation/fang}
}

@inproceedings{baruch2019little,
  author    = {G. Baruch and M. Baruch and Y. Goldberg},
  title     = {A Little is Enough: Circumventing Defenses for Distributed Learning},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2019},
  url       = {https://arxiv.org/abs/1902.06156}
}

@inproceedings{bagdasaryan2020backdoor,
  author    = {E. Bagdasaryan and A. Veit and Y. Hua and D. Estrin and V. Shmatikov},
  title     = {Backdoor Attacks on Federated Learning},
  booktitle = {Proc. 23rd Int. Conf. Artificial Intelligence and Statistics (AISTATS)},
  year      = {2020},
  url       = {https://arxiv.org/abs/1807.00459}
}

@inproceedings{wang2021attack,
  author    = {Y. Wang and Y. Hu and G. Li and Z. Lin},
  title     = {Attack of the Tails: Yes, You Really Can Backdoor Federated Learning},
  booktitle = {Proc. USENIX Security Symp.},
  year      = {2021},
  url       = {https://www.usenix.org/conference/usenixsecurity21/presentation/wang-yue}
}

@inproceedings{xie2020dba,
  author    = {C. Xie and K. Huang and P. Chen and B. Li},
  title     = {DBA: Distributed Backdoor Attacks Against Federated Learning},
  booktitle = {Proc. Int. Conf. Learning Representations (ICLR)},
  year      = {2020},
  url       = {https://openreview.net/forum?id=rkgyS0VFvr}
}

@inproceedings{zhu2019deep,
  author    = {L. Zhu and Z. Liu and S. Han},
  title     = {Deep Leakage from Gradients},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2019},
  url       = {https://papers.nips.cc/paper/2019/hash/60a6c4002cc7b29142def8871531281a-Abstract.html}
}

@inproceedings{geiping2020inverting,
  author    = {J. Geiping and H. Bauermeister and H. Dr{\"o}ge and M. Moeller},
  title     = {Inverting Gradients: How Easy Is It to Break Privacy in Federated Learning?},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.14053}
}

@inproceedings{nasr2019comprehensive,
  author    = {M. Nasr and R. Shokri and A. Houmansadr},
  title     = {Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks Against Centralized and Federated Learning},
  booktitle = {IEEE Symp. Security and Privacy (S\&P)},
  year      = {2019},
  url       = {https://ieeexplore.ieee.org/document/8835245}
}

@inproceedings{melis2019exploiting,
  author    = {L. Melis and C. Song and E. De Cristofaro and V. Shmatikov},
  title     = {Exploiting Unintended Feature Leakage in Collaborative Learning},
  booktitle = {IEEE Symp. Security and Privacy (S\&P)},
  year      = {2019},
  url       = {https://ieeexplore.ieee.org/document/8835244}
}

@inproceedings{bonawitz2017practical,
  author    = {K. Bonawitz and V. Ivanov and B. Kreuter and A. Marcedone and H. B. McMahan and S. Patel and D. Ramage and A. Segal and K. Seth},
  title     = {Practical Secure Aggregation for Privacy-Preserving Machine Learning},
  booktitle = {Proc. ACM Conf. Computer and Communications Security (CCS) Workshop},
  year      = {2017},
  url       = {https://arxiv.org/abs/1705.10816}
}


@inproceedings{chen2022calfat,
  author    = {C. Chen and Y. Liu and X. Ma and L. Lyu},
  title     = {CalFAT: Calibrated Federated Adversarial Training with Label Skewness},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2022},
  url       = {https://openreview.net/forum?id=H8wCqfY3h8}
}

@inproceedings{goodfellow2015explaining,
  author    = {I. J. Goodfellow and J. Shlens and C. Szegedy},
  title     = {Explaining and Harnessing Adversarial Examples},
  booktitle = {Proc. Int. Conf. Learning Representations (ICLR)},
  year      = {2015},
  url       = {https://arxiv.org/abs/1412.6572}
}

@inproceedings{madry2018towards,
  author    = {A. Madry and A. Makelov and L. Schmidt and D. Tsipras and A. Vladu},
  title     = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  booktitle = {Proc. Int. Conf. Learning Representations (ICLR)},
  year      = {2018},
  url       = {https://arxiv.org/abs/1706.06083}
}
