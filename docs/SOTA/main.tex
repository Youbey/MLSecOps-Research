\documentclass{article}
\usepackage{graphicx} 
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage{float}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{pdflscape}
\usepackage{geometry}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{float}



\title{Federated Learning under Adversaries: State-of-the-art Attacks and  Defenses for MLSecOps - test}
\author{
Ayoub Bendraou \and
Aya Fsahi \and
Nazih Ouchta \and
Salim Ghoudane
}
\date{November 2025}



\begin{document}


\maketitle

\vspace{2.5cm}

\begin{abstract}
Federated Learning (FL) has emerged as a paradigm that enables collaborative model training across distributed clients without centralizing sensitive data, offering significant privacy and compliance advantages over conventional machine learning. However, its decentralized nature introduces new attack surfaces and complex adversarial behaviors across data, model, communication, and aggregation layers. This state-of-the-art (SOTA) report situates FL within the broader context of Machine Learning Security Operations (MLSecOps) by mapping the evolution of attack taxonomies and defense mechanisms. It presents an integrated overview of integrity threats such as poisoning and backdoor attacks, privacy risks including gradient inversion and membership inference, communication-layer exploits like Sybil and man-in-the-middle attacks, and aggregator-level vulnerabilities such as model replacement and free-riding. Building on these foundations, the work surveys SOTA defenses—ranging from Byzantine-robust aggregation rules (Krum, Bulyan, FLTrust) and adaptive anomaly detection to differential privacy, secure aggregation, and federated adversarial training. The analysis emphasizes the trade-offs between privacy, robustness, and scalability, highlighting emerging trends in layered, privacy-preserving defense frameworks and MLSecOps pipelines capable of anticipating and autonomously mitigating threats throughout the FL lifecycle.
\end{abstract}

\newpage

\tableofcontents

\newpage


\section{What is Federated Learning?}
\textbf{Federated Learning (FL)} is formally defined and described across the literature as:
\begin{itemize}

\item A machine learning setting where \textbf{many clients} (e.g., mobile devices or whole organizations) \textbf{collaboratively train a model} under the orchestration of a central server (e.g., service provider), while \textbf{keeping the training data decentralized} \cite{ref16}. 


\item A learning paradigm introduced in 2016 by McMahan et al. where the learning task is solved by a loose federation of participating devices (clients) coordinated by a central server \cite{ref16}. 


\item A mechanism that embodies the principles of \textbf{focused collection and data minimization} \cite{ref16}. Client raw data is stored locally and is explicitly not exchanged or transferred \cite{ref16}. Instead, \textbf{focused updates intended for immediate aggregation} are used to achieve the learning objective \cite{ref16}. 

\item A model that mitigates many of the \textbf{systemic privacy risks and costs} resulting from traditional, centralized Machine Learning (ML) and data science approaches \cite{ref16}. 


\item A \textbf{privacy-preserving decentralized approach}, which keeps raw data on devices and involves local ML training while eliminating data communication overhead \cite{ref13}. A federation of the learned and shared models is then performed on a central server to aggregate and share the built knowledge among participants \cite{ref13}. 


\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{4-lifecycle.png}
    \caption{the various actors in a federated learning system \cite{ref16}}
    \label{fig:placeholder}
\end{figure}






\subsection{Motives and Need for Federated Learning}
The necessity for FL arose from the convergence of several technology trends and inherent failures within previous machine learning (ML) architectures, particularly regarding data privacy and scalability.
\subsubsection{The Data Revolution and AI's Needs}
The first motive is the \textbf{unprecedented availability of data} generated by connected devices, such as smartphones, Internet-of-Things (IoT) devices, and wearable medical devices
. These modern mobile devices possess powerful sensors (cameras, microphones, GPS) and are frequently carried, giving them access to a \textbf{wealth of data}
.
Since ML models are \textbf{data hunger}
 and Deep Learning (DL) applications have experienced vigorous growth, models trained on this rich, real-world data hold the promise of \textbf{greatly improving usability} by powering more intelligent applications, such as improving speech recognition and text entry
.
\subsubsection{Centralized Learning Failures and Privacy Mandates}
The conventional ML architecture relies on a \textbf{cloud-centric architecture} where data is continuously streamed, centrally stored, and processed
. This centralized model faced severe limitations:
\begin{itemize} \item \textbf{High Risk of Disclosure:} The \textbf{sensitive nature of the data} means there are risks and responsibilities to storing it centrally
. Without serious privacy consideration, sensitive data is \textbf{highly exposed to disclosure, attacks, and cyber risks}. Breaches like Equifax, Marriott, and eBay demonstrated these risks in recent memory. \item \textbf{Regulatory Compliance Challenges:} Stringent regulations like the European Union’s \textbf{GDPR} and the \textbf{HIPAA} guidelines strictly limit data sharing and storage. These acts challenge centralized models, as patient personal indicators cannot be shared without consent, meaning centralized AI-driven health analytics models are challenged with the quality of shared data (often anonymized, yielding generic interpretability). \item \textbf{Cost and Latency:} Centralized practices engage \textbf{unacceptable latency and high cost}
. \end{itemize}
\subsubsection*{1.3. On-Device Learning Isolation}
While running ML models locally on devices (on-site ML) solves the data transfer problem, it creates a new drawback: the local models are limited to \textbf{each user’s experience} and do not \textbf{benefit from peers' data}
.
\paragraph{FL as the Solution:}
FL was developed to enable users to \textbf{collectively reap the benefits of shared models} trained from rich data, without the need to centrally store it
. This approach allows the decoupling of model training from the need for direct access to the raw training data
.

\subsection{Mechanism: How does Federated Learning work?}
The standard FL workflow is orchestrated by a central server and is often based on the \textbf{Federated Averaging (FedAvg) algorithm} \cite{ref16, ref13}.
\subsubsection*{Architecture and Workflow}
The FL life cycle consists of several continuous communication rounds until the global model reaches the desired accuracy \cite{ref13}.

\begin{enumerate} 

\item \textbf{Client Selection/Initiation:} The server generates or initializes a generic model \cite{ref13}. In each round, a subset of clients is sampled \cite{ref16}. Typical eligibility requirements include the device being \textbf{in charge, idle, and on an unmetered connection} (i.e., Wi-Fi) \cite{ref13}. Advanced protocols like \textbf{FedCS} may require clients to report information about their resources (upload and update time) to a Mobile-Edge Computing (MEC) server, which then determines the subset able to complete the FL steps within a deadline \cite{ref13}. 

\item \textbf{Broadcast and Download:} Selected clients download the current model parameters/weights and a training program (e.g., a TensorFlow graph) from the server \cite{ref16, ref13}. The global updates might propagate to local nodes at different time instants depending on communication channel specifics \cite{ref15}. 



\item \textbf{Local Training:} Each device locally trains and optimizes the global model using its local data \cite{ref13}. This involves running SGD \cite{ref13}. 

\item \textbf{Upload:} The client sends its locally computed parameters/updates to the central server \cite{ref13}. 


\item \textbf{Aggregation:} The server collects and aggregates the updates. This process is often a \textbf{weighted average} based on client dataset size \cite{ref13, ref15}. 

\item \textbf{Model Update:} The server uses the aggregated gradient to update the global model. This new shared model is distributed back to the clients, and the process repeats until convergence \cite{ref13, ref15}. \end{enumerate} In this star network communication topology, a subset of selected devices performs local training on their \textbf{non-identically-distributed user data} and sends these local updates to the server \cite{ref14}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{3.training process.png}
    \caption{FL training process.}
    \label{fig:placeholder}
\end{figure}


\subsubsection{Alternative Learning Paradigms and Techniques} 

\begin{itemize} 

\item \textbf{Fully Decentralized / Peer-to-Peer Distributed Learning:} This approach removes the need for a central server coordinating the overall computation, with devices communicating directly with neighbors \cite{ref16}. 


\item \textbf{FL Algorithms for Aggregation:} \begin{itemize} 

\item \textbf{FedPer (Federated learning with personalization layer):} Clients send base layers (for representational learning) to the server but retain higher layers (decision specifics) locally \cite{ref15}. 

\item \textbf{FedMA (Federated learning with matched averaging):} Constructs a global model by averaging values of hidden layers (e.g., convolutional layers) \cite{ref15}. 

\item \textbf{FedDist (Federated Distance):} Computes the distance between neurons with similar features, suitable for sparse data \cite{ref15}. 

\item \textbf{One-shot FL:} The massive number of communication rounds is substituted with only \textbf{one round} where each device trains its model until completion, and models are aggregated using ensemble learning techniques \cite{ref13}. \end{itemize} 

\item \textbf{Data Partitioning:} 


\begin{itemize} \item \textbf{Horizontal FL (HFL):} Local clients have the \textbf{same feature sets} but in \textbf{different data spaces} \cite{ref15}. \item \textbf{Vertical FL (VFL):} Local clients have the \textbf{same sample space} but \textbf{different feature sets} \cite{ref15}. \item \textbf{Transfer Learning-based FL (TL-FL):} Extends VFL to include more local clients with different feature sets but the same sample space, extracting common features to a shared space, suitable for EHRs collected from heterogeneous sources \cite{ref15}. \end{itemize} \end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{data part.png}
    \caption{FL-HI aggregation conceptual overview: The left figure depicts the horizontal FL, centre figure depicts the vertical FL type, and the right
figure indicates the transfer learning scheme in FL \cite{ref15}}
    \label{fig:placeholder}
\end{figure}



\section{Technical overview}

\subsection{Federated Optimization Problem Formulation}
FL focuses on supervised machine learning (ML), where the goal is to find model parameters $\mathbf{w}$ (a vector) that minimize a global loss function $F(\mathbf{w})$ over a massive, partitioned dataset.

\paragraph{The Global Objective Function (Non-IID Setting)}
The overall optimization goal is to minimize the finite-sum objective function $F(\mathbf{w})$,
which is a weighted sum of the local loss functions $F_k(\mathbf{w})$ across $K$ clients:
\[
\min_{\mathbf{w}} F(\mathbf{w}), \quad \text{where  }   \textcolor{white}{-} F(\mathbf{w}) \stackrel{\text{def}}{=} \sum_{k=1}^{K} \frac{n_k}{n} F_k(\mathbf{w})
\]
Where:
\begin{itemize}
    \item $K$ is the total number of clients participating in the learning rounds.
    \item $n_k$ is the number of data samples on client $k$, where $n_k = |P_k|$ ($P_k$ is the partition of data assigned to client $k$).
    \item $n$ is the total number of samples across all clients, where $n = \sum_{k=1}^{K} n_k$.
    \item $F_k(\mathbf{w})$ is the \textbf{local objective function} (empirical risk) for the $k$th client, typically defined as $F_k(\mathbf{w}) = \frac{1}{n_k} \sum_{i \in P_k} f_i(\mathbf{w})$, where $f_i(\mathbf{w})$ is the loss function for the $i$th data sample $(x_i, y_i)$.
\end{itemize}

\subsection{The Federated Averaging (FedAvg) Algorithm}
FedAvg is the chosen practical algorithm for FL implementation, combining local Stochastic Gradient Descent (SGD) on each client with a server that performs iterative model averaging.
The algorithm is designed to use additional local computation to \textbf{reduce the number of communication rounds} by $10$--$100\times$ compared to synchronized SGD.

The algorithm uses three key parameters to control computation:
\begin{itemize}
    \item $C$: The fraction of clients selected to perform computation in each round.
    \item $E$: The number of local training passes (epochs) each selected client makes over its local dataset in one communication round.
    \item $B$: The local minibatch size used for client updates. ($B = \infty$ means the full local dataset is used as one minibatch).
\end{itemize}
The number of local updates per round for client $k$ is $u_k = E \frac{n_k}{B}$.

\paragraph{FedAvg Pseudocode (Adapted from Algorithm 1)}
\hrule
\vspace{0.1cm}
\noindent \textbf{Server Executes:}
\begin{enumerate}
    \item Initialize global model weights $\mathbf{w}_0$.
    \item \textbf{For each round $t = 1, 2, \ldots, T$ do:}
    \begin{itemize}
        \item $m \leftarrow \max(C \cdot K, 1)$ (Number of clients selected).
        \item $S_t \leftarrow$ (random set of $m$ clients).
        \item \textbf{For each client $k \in S_t$ in parallel do:} $\mathbf{w}_k^{t+1} \leftarrow \text{ClientUpdate}(k, \mathbf{w}_t)$
        \item \textbf{Aggregate (Weighted Average):} The new global model $\mathbf{w}_{t+1}$ is calculated as the weighted sum of the returned local models, where $\sum_{k=1}^{K} \frac{n_k}{n} = 1$.
        \[
        \mathbf{w}_{t+1} \leftarrow \sum_{k \in S_t} \frac{n_k}{n} \mathbf{w}_k^{t+1}
        \]
    \end{itemize}
\end{enumerate}
\vspace{0.1cm}
\noindent \hrule
\vspace{0.1cm}
\noindent \textbf{ClientUpdate($k$, $\mathbf{w}$):} (\textbf{Run on client $k$})
\begin{enumerate}
    \item $\mathcal{B} \leftarrow$ (split local dataset $P_k$ into batches of size $B$).
    \item \textbf{For each local epoch $i$ from 1 to $E$ do:}
    \begin{itemize}
        \item \textbf{For batch $b \in \mathcal{B}$ do:} $\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla \ell(\mathbf{w}; b)$
    \end{itemize}
    \item return $\mathbf{w}$ to server.
\end{enumerate}
\vspace{0.1cm}
\hrule

\subsubsection*{FedSGD as a Baseline}
FedAvg generalizes \textbf{Federated SGD (FedSGD)}, which is the simplest synchronous update scheme.
FedSGD corresponds exactly to FedAvg when $B = \infty$ (full local batch) and $E = 1$ (one local epoch).

\begin{itemize}
    \item In FedSGD, each client $k$ computes $\mathbf{g}_k = \nabla F_k(\mathbf{w}_t)$, the average gradient on its local data.
    \item The server aggregates the gradients and applies the update $\mathbf{w}_{t+1} \leftarrow \mathbf{w}_t - \eta \sum_{k=1}^{K} \frac{n_k}{n} \mathbf{g}_k$.
    \item This is mathematically equivalent to the model averaging: $\forall k, \mathbf{w}_k^{t+1} \leftarrow \mathbf{w}_t - \eta \mathbf{g}_k$ and then $\mathbf{w}_{t+1} \leftarrow \sum_{k=1}^{K} \frac{n_k}{n} \mathbf{w}_k^{t+1}$.
\end{itemize}


\section{Applications}
FL is deployed across massive consumer products (Cross-Device) and organizations with high confidentiality needs (Cross-Silo).
\subsubsection*{Consumer, Mobile, and Edge Devices (Cross-Device FL)} \begin{itemize} \item \textbf{Mobile Keyboards:} FL was first tested on Google's virtual keyboard, \textbf{Gboard} \cite{ref13}. It is used to improve \textbf{next-word prediction}, \textbf{query suggestion}, word completion, corrections \cite{ref13}, and \textbf{emoji prediction} \cite{ref13}. FL is also adapted to learn \textbf{out-of-vocabulary (OOV) words} \cite{ref13}. \item \textbf{Mobile Operating Systems:} Used for features on \textbf{Pixel phones} and in \textbf{Android Messages} \cite{ref16}. Apple uses cross-device FL in \textbf{iOS 13} for applications like the \textbf{QuickType keyboard} and the \textbf{vocal classifier for “Hey Siri”} \cite{ref16}. \item \textbf{General Mobile Apps:} Powers applications such as face detection and voice recognition \cite{ref14}. Explored for \textbf{hotword detection} \cite{ref16}. \item \textbf{IoT Systems:} Used to limit vulnerabilities in large-scale IoT systems \cite{ref13}. Use cases include \textbf{intrusion detection systems} based on anomaly detection \cite{ref13} and \textbf{Mobile Edge Computing (MEC)} optimization \cite{ref13}. \item \textbf{Electric Vehicles (EVs):} Analyzing driver behavior metrics (using LSTM) to predict battery failure \cite{ref13}. \end{itemize}
\subsubsection*{Organizational and Industrial (Cross-Silo FL)} \begin{itemize} \item \textbf{Healthcare Informatics (HI):} FL is a strong fit for HI, balancing patient privacy with ML by keeping patient data on-premise \cite{ref13}. Applications include: \begin{itemize} \item Predicting mortality and hospital stay time using distributed \textbf{Electronic Health Records (EHRs)} \cite{ref13}. \item Predicting hospitalizations for heart disease patients \cite{ref13}. \item \textbf{Medical image prediction} (e.g., brain tumor segmentation) \cite{ref13, 15}. \item Collaborative learning for multi-modal \textbf{COVID-19 diagnosis} with X-ray and Ultrasound imagery \cite{ref15}. \item Detecting \textbf{adverse events in mass-scale vaccination programs} \cite{ref15}. \item Analysis of biomedical data (e.g., subcordinal brain changes in neurological disease) \cite{ref13}. \item Drug discovery (e.g., MELLODDY project) \cite{ref15}. \item Training on \textbf{multi-institutional datasets} constrained by HIPAA/FERPA \cite{ref16}. \end{itemize} \item \textbf{Finance:} \textbf{Finance risk prediction for reinsurance} \cite{ref16}. Collaborative training for \textbf{fraud detection} models \cite{ref16}. Used by Webank and Swiss Re for high-precise financial analysis \cite{ref15}. \item \textbf{Recommender Systems:} Generating personalized recommendations using collaborative filtering \cite{ref13}. \item \textbf{Online Retailers:} Analyzing user click-stream data to enhance prediction of consumer's next browsing activities \cite{ref13}. \item \textbf{Model Types:} Applicable to models that operate over large inventories where clients interact with only a tiny fraction of items (sparse problems), such as \textbf{natural language models} or \textbf{content ranking models} using an embedding lookup table \cite{ref16}. \end{itemize}



\section{Advantages}
FL provides significant advantages over traditional centralized and on-device machine learning models, primarily related to privacy, cost, and system efficiency.
\begin{itemize} \item \textbf{Data Privacy and Security Guarantees:} FL is a \textbf{privacy-preserving decentralized approach} that keeps \textbf{raw data on-devices} and precludes direct access to it \cite{ref13}. The approach involves sharing focused model updates (e.g., gradients) instead of the raw data \cite{ref14}. Local training preserves the privacy, confidentiality, and integrity of patient data \cite{ref15}. \item \textbf{Compliance and Risk Mitigation:} FL can \textbf{mitigate many of the systemic privacy risks and costs} resulting from traditional centralized approaches \cite{ref16}. It effectively handles the tradeoff between model learning and regulatory compliance (e.g., HIPAA/GDPR) because raw data is not centralized \cite{ref15}. \item \textbf{Communication and Network Efficiency:} FL eliminates the data communication overhead associated with centralized systems \cite{ref13}. By pushing computation to the edge and requiring only small, iterative model updates, it reduces strain on the network \cite{ref14}. \item \textbf{Knowledge Sharing (vs. On-Device ML):} It overcomes the limitation of isolated on-device ML (where models don't benefit from peers' data) by aggregating local models to \textbf{share knowledge} among participants \cite{ref13}. \item \textbf{Accuracy and Model Diversity:} FL achieves \textbf{high precision and accuracy} by leveraging a large volume of data across many clients \cite{ref13}. It also enables training on \textbf{multi-institutional datasets} (e.g., medical, education) where centralization was constrained, potentially leading to improved model \textbf{fairness} and diversity \cite{ref16}. \item \textbf{Resource and Latency Reduction:} FL handles the challenge of expensive centralized training \cite{ref15}. It resolves the network latency problem as clients process data locally, eliminating the need to fetch data from a remote server \cite{ref13}. \end{itemize}




























\newpage
\section{Taxonomy of Attacks in Federated Learning}
\subsection{Overview}
\hspace{1.3em} Federated Learning (FL) introduces distributed model training across multiple clients without centralizing data. While this paradigm preserves data locality, it exposes the system to unique security and privacy threats that differ from traditional centralized ML settings. These attacks can be categorized into five principal groups: Integrity, Privacy, Communication, Aggregator/Server, and Adversarial or Robustness attacks (adapted from Lyu et al., 2022; Nguyen et al., 2023).

Figure~\ref{fig:fl-taxonomy} provides an overview of the main attack surfaces in FL, grouped by layer: Integrity, Privacy, Communication, Aggregation, and Robustness.

\begin{figure}[H]
\centering
\rotatebox{90}{\includegraphics[width=0.8\linewidth, angle=270]{cartographie.png}}
\caption{Taxonomy of Attacks and Defenses in Federated Learning}
\label{fig:fl-taxonomy}
\end{figure}

\subsection{Integrity Attacks}
\hspace{1.3em} Integrity attacks aim to compromise the correctness or reliability of the global model. Rather than stealing information, the adversary manipulates local updates to degrade model performance or introduce hidden behaviors. Typical examples include data poisoning—where malicious samples are injected into a client’s dataset—and model poisoning, in which attackers directly modify the local model gradients before aggregation. Backdoor attacks constitute a more subtle variant: they embed a hidden trigger (e.g., a specific pixel pattern or phrase) that causes targeted misclassification while preserving overall accuracy.
These threats are particularly difficult to detect in FL because of the asynchronous, privacy-preserving nature of local updates. (Bagdasaryan et al., 2020; Fung et al., 2020)

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{FL_Poisoning_Data.png}
\caption{The attacker compromises one or more participants, trains on the backdoor data using the constrain-and-scale technique, and submits the resulting model, which replaces the joint model as the result of federated averaging. (Bagdasaryan et al., 2020; Fung et al., 2020)}
\label{fig:integ-attacks}

\end{figure}

\subsubsection{Model poisoning}
A compromised participant or a small group can directly modify the weights sent to the server so that the global model learns a hidden functionality (backdoor), while preserving performance on the main task.
\begin{itemize}
    \item Unlike \textit{data poisoning} where training examples are altered, the attacker manipulates the local model and the update that will be aggregated.
    \item The FL design (local updates, aggregation) gives participants direct influence and therefore an attack surface.
\end{itemize}

\paragraph{Main method — Model Replacement}
The attacker trains a malicious local model \(X\) that encodes the backdoor. They then construct the following submission:
\[
\tilde{L} = \gamma \cdot (X - G_t) + G_t
\]
\begin{itemize}
    \item \(G_t\): current global model.
    \item \(X\): locally trained backdoor model.
    \item \(\gamma\): scaling factor (chosen to compensate for dilution by aggregation of the average).
\end{itemize}

\noindent \textbf{Logic:} the aggregator computes a weighted average of the updates. For the malicious contribution to dominate the average, the attacker amplifies their update (\(\gamma\)) so that the resulting average is close to \(X\). This enables \textit{single-shot} injection in a single round if the update is large enough.

\noindent \textbf{Limitations:} a highly amplified update is theoretically detectable if the server observed individual contributions or their norms (but \textit{Secure Aggregation} often prevents this visibility). Mechanisms such as \textit{clipping} or \textit{DP} can reduce the effect of large contributions.

\paragraph{Evasion: Constrain-and-Scale}
To evade defenses based on anomaly detection, the attacker does not simply apply a post-training scale. They train \(X\) by minimizing a multi-objective loss:
\[
L = \alpha L_{\text{class}} + (1-\alpha) L_{\text{ano}}
\]
\begin{itemize}
    \item \(L_{\text{class}}\): the standard loss (main task + backdoor); trains the model to perform both the main task and the backdoor.
    \item \(L_{\text{ano}}\): penalizes anomaly (e.g., high \(L_2\) norm, low cosine similarity with benign update direction, drop in accuracy on an audit dataset).
    \item \(\alpha\): trade-off between strength and stealth.
\end{itemize}

\noindent \textbf{Goal:} produce individual updates that look like benign updates in magnitude and direction while enabling, once combined, the desired global effect.

\paragraph{Moment of injection}
\begin{itemize}
    \item Early attacks, before convergence, are more easily forgotten.
    \item Late attacks, near convergence, are much more persistent, because participants' updates become small and cancel out, leaving the backdoor intact.
\end{itemize}

\paragraph{Persistence and factors of effectiveness}
Persistence depends on: the rarity of the trigger in benign data, local/global learning rates, and the number of participants per round.

\subsubsection{Distributed Backdoor Attacks (DBA)}
DBA — \textit{Distributed Backdoor Attacks}. Xie et al. present a backdoor attack specifically designed to exploit the distributed and heterogeneous nature of Federated Learning. Unlike classic attacks where the same trigger is injected locally by one (or several) client(s), DBA splits the global trigger into sub-triggers implanted by different clients. Each adversarial client introduces only a small part of the pattern; federated aggregation (FedAvg) then reassembles the backdoor at the global model level. This strategy makes the attack stealthier (local updates appear close to benign updates) and harder to detect by outlier-detection methods or by filtering large updates.

\paragraph{Differences with classic backdoor}
\begin{itemize}
    \item \textbf{Nature of the trigger:} classic backdoor — global trigger; DBA — distributed trigger, decomposed into several sub-patterns \(\{\phi_i^\ast\}\) spread across multiple clients.
    \item \textbf{Strategy to dominate aggregation:} classic backdoor often uses model replacement (strong scaling of updates, large \(\gamma\)); DBA favors distributing the signal: each client sends only a small contribution.
    \item \textbf{Stealth / detection:} classic backdoor can be spotted if the aggregator monitors update magnitude or distance; DBA is better camouflaged because each individual update resembles a legitimate update.
    \item \textbf{Threat model required:} classic backdoor — one or a few compromised clients suffice; DBA — requires several compromised clients participating in targeted rounds.
    \item \textbf{Robustness to defenses:} classic backdoor can be mitigated by clipping, robust aggregation, or anomalous update detection; DBA circumvents several defenses designed for centralized backdoors.
\end{itemize}

\paragraph{Mathematical formalism}
% Federated objective
\[
\min_w F(w)=\frac{1}{N}\sum_{i=1}^N f_i(w)
\]

% FedAvg aggregation (round t)
\[
G^{t+1}=G^t+\eta\frac{1}{n}\sum_{i=1}^n (L_i^{t+1}-G^t)
\]

% classic backdoor attack: model replacement (scaling)
\[
L_i^{t+1} \leftarrow \gamma\,(w_i - G^t) + G^t \quad (\gamma \text{ large})
\]

% DBA attack: each attacker i optimizes locally with its sub-trigger \phi_i^*
\[
w_i^\ast = \arg\max_{w_i}\; \Big( \sum_{j\in S^{i}_{poi}} \Pr\big[G^{t+1}(R(x^i_j,\phi_i^\ast))=\tau\big] + \sum_{j\in S^{i}_{cln}} \Pr\big[G^{t+1}(x^i_j)=y^i_j\big] \Big)
\]

DBA introduces the decomposition \(\phi \to \{\phi_i^\ast\}\): the \(\phi_i^\ast\) are designed to be weak locally but cooperate to produce a global trigger after aggregation.

\subsubsection{Sybil attacks}
A \textbf{sybil} is a fake or multiple client identity controlled by the same attacker. Objective: amplify the malicious influence on the global aggregate by multiplying contributions.

\paragraph{Types of Sybil-related attacks}
\begin{itemize}
  \item \textbf{Model poisoning}: sending malicious gradients to bias the global model.
  \item \textbf{Label-flipping}: flipping local labels.
  \item \textbf{Backdoor}: injecting a discrete trigger causing misclassification when the trigger is present.
\end{itemize}

\paragraph{Why it is dangerous}
\begin{itemize}
  \item A single real attacker can spawn \(n\) sybils and multiply their influence.
\end{itemize}

\subsection{Privacy Attacks}
\hspace{1.3em} Privacy attacks focus on recovering or inferring information about clients’ training data from shared model parameters or gradients. Despite not having access to raw data, attackers can exploit mathematical properties of the gradients to reconstruct private inputs (gradient inversion, Zhu et al., 2019) or determine whether a specific data point was used in training (membership inference). More sophisticated approaches—such as property inference—can deduce aggregate demographic or categorical information about local datasets.
These attacks demonstrate that FL alone does not guarantee privacy; without cryptographic protection or differential privacy, model updates remain vulnerable to inversion and inference analysis. (Melis et al., 2019)

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth, angle=]{Privacy_attack.png}
\caption{The overview of the DLG algorithm. Variables to be updated are marked with a bold border. While normal participants calculate deltaW to update parameter using its private training data, the malicious attacker updates its dummy inputs and labels to minimize the gradients distance. When the optimization finishes, the evil user is able to steal the training data from honest participants. (Zhu et al. 2019)}
\label{fig:privacy-att}
\end{figure}

\subsection{Communication Attacks}
\hspace{1.3em} In communication attacks, the adversary targets the exchange channel between clients and the central server. Since FL involves frequent transmission of model updates, a man-in-the-middle or replay attacker can intercept, modify, or resend parameter messages. Eavesdropping on updates may reveal sensitive information about gradients or hyperparameters.
Securing FL communication requires authenticated and encrypted channels, but even then, timing or metadata side-channels may leak information (Lyu et al., 2022).

\subsection{Aggregator / Server Attacks}
\hspace{1.3em} In a standard FL setting, the server is assumed to be semi-honest—but this assumption may not hold in adversarial contexts. A malicious or compromised server can perform model replacement (substituting the aggregated model with a poisoned version) or malicious aggregation (selectively weighting updates to bias outcomes). Conversely, even honest servers can unintentionally enable these attacks if they rely on naive averaging (FedAvg) without anomaly detection or robust aggregation.
Hence, secure aggregation protocols and verifiable model updates are crucial to maintaining integrity at the central level. (Bhagoji et al., 2019)

\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{Server_attack.png}
\caption{A typical FL training process, in which both the (potentially malicious) FL server/aggregator and malicious participants
may compromise the FL system. (Threats to Federated Learning: A Survey, Lyu et al.)}
\label{fig:server-att}
\end{figure}

\subsection{Adversarial / Robustness Attacks}
\hspace{1.3em} Beyond the training process itself, adversarial or robustness attacks target the learned model’s behavior during inference. Attackers craft adversarial examples—inputs with imperceptible perturbations that cause misclassification or exploit model drift and free-riding phenomena, where participants benefit from others’ updates without contributing valid gradients.
These attacks highlight that FL systems must combine robustness evaluation, adversarial training, and continuous monitoring to maintain resilience against both internal and external threats. (Nguyen et al., 2023)

\begin{landscape}
\begin{table}[ht]
\centering
\small
\caption{Comprehensive Mapping of Federated Learning Attacks}
\begin{tabularx}{1.2\textwidth}{l l X X}
\toprule
\textbf{Category} & \textbf{Subtype} & \textbf{Description} & \textbf{Key References} \\
\midrule
\textbf{Poisoning Attacks} & Data Poisoning & Manipulation of local datasets to bias global model updates & \cite{bagdasaryan2020backdoor,fang2020local,blanchard2017machine} \\
& Model Poisoning & Direct modification of gradients or model weights to introduce hidden behavior or degradation & \cite{bhagoji2019analyzing,baruch2019little} \\
\midrule
\textbf{Backdoor Attacks} & Trigger-based Backdoors & Introduction of hidden malicious patterns (triggers) that cause misclassification when activated & \cite{bagdasaryan2020backdoor} \\
\midrule
\textbf{Inference Attacks} & Membership Inference & Determining whether specific data points were used in training & \cite{nasr2019comprehensive,melis2019exploiting} \\
& Property Inference & Inferring sensitive attributes of users’ data from gradients or model parameters & \cite{melis2019exploiting} \\
& Gradient Inversion & Reconstructing training samples from shared gradients & \cite{zhu2019deep,geiping2020inverting} \\
\midrule
\textbf{Communication Attacks} & Man-in-the-Middle (MITM) & Intercepting or modifying updates in transit between clients and server & \cite{lyu2020threats} \\
& Sybil Attacks & A single adversary simulates multiple fake clients to bias aggregation & \cite{fung2018mitigating} \\
\midrule
\textbf{Free-rider Attacks} & Model Theft & Participants submit fake updates but still benefit from global model & \cite{wang2021attack} \\
\midrule
\textbf{Model Replacement} & Model Overwrite & Substituting the global model with a maliciously crafted one & \cite{bagdasaryan2020backdoor} \\
\bottomrule
\end{tabularx}
\end{table}
\end{landscape}


% \begin{landscape}
% \subsection{Representative Works}
% Table~\ref{tab:fl-attacks} summarizes key representative works, their publication venues, and their classification in the CORE database.

% \begin{table}[h!]
% \centering
% \footnotesize
% \begin{tabular}{|p{3cm}|p{5cm}|p{3.5cm}|p{2cm}|p{6cm}|}
% \hline



% \textbf{Attack Type} & \textbf{Representative Paper} & \textbf{Venue} & \textbf{CORE Rank} & \textbf{Description} \\ \hline
% Data Poisoning & Bagdasaryan et al., 2020 & AISTATS & A & Demonstrates model-replacement and backdoor attacks targeting the global model aggregation in FL. \\ \hline
% Gradient Leakage / Inference & Zhu et al., 2019 & NeurIPS & A* & Shows that private training data can be reconstructed from shared gradients. \\ \hline
% Property / Membership Inference & Melis et al., 2019 & IEEE S\&P & A* & Reveals feature leakage through model updates during collaborative training. \\ \hline
% Privacy Analysis & Nasr et al., 2019 & IEEE S\&P & A* & Formalizes white-box inference attacks against centralized and federated models. \\ \hline
% Byzantine / Robustness & Blanchard et al., 2017 & NeurIPS & A* & Introduces Byzantine-tolerant gradient aggregation methods for adversarial clients. \\ \hline
% Sybil / Collusion Attacks & Fung et al., 2020 & RAID & A & Demonstrates Sybil-based poisoning and proposes FoolsGold defense. \\ \hline
% Distributed Backdoors & Xie et al., 2020 & ICLR & A* & Introduces DBA attacks — coordinated small-scale poisoning to evade detection. \\ \hline
% GAN-based Reconstruction & Hitaj et al., 2017 & ACM CCS & A* & Early work using GANs to extract data from collaborative learning setups. \\ \hline
% \end{tabular}
% \caption{Mapping of Attacks in Federated Learning: representative works and venues.}
% \label{tab:fl-attacks}
% \end{table}
% \end{landscape}


% \begin{thebibliography}{9}

% \bibitem{mcm2017}
% McMahan, H. B., Moore, E., Ramage, D., Hampson, S., y Arcas, B. A. (2017). 
% \textit{Communication-Efficient Learning of Deep Networks from Decentralized Data}. 
% Proceedings of AISTATS.

% \bibitem{bagdasaryan2020how}
% E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, V. Shmatikov,
% \textit{How to backdoor federated learning},
% Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS), Cornell Tech, 2020.

% \bibitem{xm2020}
% Xie, C., Koyejo, O., Gupta, I. (2020). 
% \textit{Distributed Backdoor Attacks Against Federated Learning}. 
% Proceedings of ICML.

% \bibitem{fung2020mitigating}
% C. Fung, C. J.M. Yoon, I. Beschastnikh,
% \textit{Mitigating Sybils in Federated Learning Poisoning},
% University of British Columbia, arXiv preprint arXiv:1808.04866, 2020.


% \bibitem{lyu2020threats}
% L. Lyu, H. Yu, Q. Yang,
% \textit{Threats to Federated Learning: A Survey},
% arXiv preprint arXiv:2005.04045, 2020.


% \end{thebibliography}




\newpage
\section{Defense Mechanisms in Federated Learning}


% \subsection{Defense context}
% \begin{itemize}
%     \item Clipping / Norm constraints: limit the magnitude of local updates.
%     \item Robust Aggregation: replace simple averaging (FedAvg) with robust functions such as Krum, Trimmed-mean, Median.
%     \item Differential Privacy (DP): noise added to updates to limit each client's influence.
% \end{itemize}

% \subsection{Defenses evaluated against DBA}
% \begin{enumerate}
%     \item \textbf{Krum}: selects the update closest to the others; DBA evades because the malicious signal is fragmented.
%     \item \textbf{Trimmed-Mean / Median}: coordinate-wise robust aggregation; DBA evades because sub-triggers are dispersed and weak.
%     \item \textbf{Performance-based detection}: testing on a small dataset; DBA evades because the global trigger appears only after combining sub-triggers.
% \end{enumerate}


% \subsection{Differential Privacy (DP)}


% Differential Privacy ($\mathbf{DP}$) is considered the \textbf{state-of-the-art model} for mathematically quantifying and limiting information disclosure concerning individuals. It provides strict privacy protection for user data [3] and is widely recognized due to its strong information-theoretic guarantees, algorithmic simplicity, and relatively small systems overhead [4].

% \subsubsection{Core Goal and Mechanism}

% The primary goal of DP is to allow the study of overall properties or cross-population patterns of a dataset without revealing information about any individual participant.

% \begin{itemize}
%     \item \textbf{Privacy by Noise:} DP is achieved by balancing the trade-off between \textbf{data utility and individual privacy} through the introduction of controlled randomness or noise into the data analysis process.
%     \item \textbf{Uncertainty Principle:} The resulting mechanism aims to introduce a level of uncertainty sufficient to mask the contribution of any single user. This ensures that attackers cannot determine whether specific individual effects are present in the dataset.
%     \item \textbf{Process Integration:} For gradient-based learning, a popular approach is to apply DP by randomly perturbing the intermediate output at each iteration . Common noise types used include Gaussian, Laplacian, or Binomial noise .
% \end{itemize}

% \subsubsection{Differential Privacy in Federated Learning (DP-FL)}

% In Federated Learning (FL), DP is used to give rigorous protections against external adversaries by ensuring that the model iterates do not overfit to any individual user's update .

% \begin{enumerate}
%     \item \textbf{Mechanism in FL:} The general approach to adding DP to iterative training procedures involves two primary steps:
%     \begin{itemize}
%         \item \textbf{Clipping:} The $\ell_2$ norm of individual client updates is bounded (clipped) . This clipping bounds the influence of each example on the overall update. Adaptive clipping adjusts the clipping threshold based on the distribution of update norms to optimize the privacy-utility trade-off.
%         \item \textbf{Noise Addition:} Gaussian noise is added to the aggregate of the clipped updates before they are applied to the global model.
%     \end{itemize}
%     \item \textbf{Privacy Scope in FL:} DP protection is typically applied at different granularity levels:
%     \begin{itemize}
%         \item \textbf{User-Level DP (UDP):} This notion of adjacency ensures that privacy holds with respect to adding or subtracting all the records of a \textbf{single client (user)}. It aims to obfuscate the relationship between model parameters and users' original data by introducing noise to the shared model prior to uploading.
%         \item \textbf{Sample-Level DP:} This approach focuses on the \textbf{record level}, protecting each individual data sample or record within a client's dataset from reconstruction attempts.
%     \end{itemize}
%     \item \textbf{Relationship to Robustness:} DP improves robustness in FL, for example, acting as a defense against data poisoning attacks [32]. Intuitively, if an adversary can only modify a few training examples (e.g., a single client's worth), DP ensures that these changes cannot cause a large shift in the distribution over learned models [32].
% \end{enumerate}

% \subsection{Homomorphic Encryption (HE)}


% Homomorphic Encryption ($\mathbf{HE}$) is a form of encryption that allows mathematical operations to be performed directly on encrypted data, known as ciphertexts, without the necessity of decrypting them beforehand [\cite{zhang2022privacy}, \cite{liu2021fedmlhe}, \cite{ref16}].

% \begin{itemize}
%     \item \textbf{Operation Equivalence:} The fundamental property of HE is that the result obtained after decrypting the computed ciphertext is \textbf{identical to the result obtained by performing the same operation directly on the original plaintext data} [\cite{zhang2022privacy}, \cite{rieke2020healthcare}].
%     \item \textbf{Types:} HE schemes range from \textbf{Partially Homomorphic Encryption} (supporting only addition or multiplication homomorphically) to \textbf{Fully Homomorphic Encryption (FHE)} [\cite{zhang2022privacy}, \cite{ref16}], which allows arbitrary arithmetic operations to be performed on the encrypted data [\cite{zhang2022privacy}].
%     \item \textbf{FL Compatibility:} Since FL model parameters often involve approximate numbers, HE methods like the Cheon-Kim-Kim-Song ($\mathbf{CKKS}$) scheme (a variant designed for approximate numbers) are used [\cite{liu2021fedmlhe}, \cite{wei2020differential}].
% \end{itemize}

% \subsubsection{HE Application for Protecting FL Models}

% In Federated Learning, although raw training data remains local, the shared model parameters or gradients can still leak sensitive client information to the central server or eavesdroppers [\cite{zhang2022privacy}, \cite{ref13}, \cite{ref14}]. HE addresses this by ensuring privacy during the communication and aggregation phases.

% \begin{enumerate}
%     \item \textbf{Comprehensive Protection:} HE-based FL (HE-FL) achieves \textbf{comprehensive privacy protection in the entire process}, including client-side data encryption and upload, server-side aggregation calculation, and global model distribution [\cite{zhang2022privacy}].
%     \item \textbf{Secure Aggregation:} The central role of HE is to enable the aggregation server to combine local model parameters (updates) without observing them in their unencrypted form [\cite{liu2021fedmlhe}, \cite{ref16}]. This cryptographic approach ensures that the gradients/weights shared between clients and the global model remain secure from potential attackers who might otherwise sniff or modify the weights [\cite{rieke2020healthcare}].
   
%     \item \textbf{Model Performance:} Because HE performs exact computations on the encrypted updates, HE-FL is considered a \textbf{lossless method} that maintains the original model performance, unlike Differential Privacy (DP) which introduces noise [\cite{liu2021fedmlhe}].
% \end{enumerate}

% \subsection{Robust Aggregation}


% \begin{itemize}
%   \item \textbf{Trimmed-mean (coordinate-wise)}: remove extreme values on each coordinate, then average.
%   \item \textbf{Coordinate-wise median}: take the median coordinate by coordinate.
%   \item \textbf{Krum / Multi-Krum}: select updates that are close to others according to Euclidean distance.
% \end{itemize}
% Limitation: they often require a bound on the number of adversaries and can be sensitive to non-IID.

% \subsection{Anomaly Detection}

% todo


\hspace{1.3em} Federated Learning defenses aim to mitigate threats without violating its core principle of decentralized data ownership. Defense research has evolved from classical Byzantine-tolerant learning toward integrated privacy-preserving and robustness-preserving frameworks. This section synthesizes the state-of-the-art (SOTA) mechanisms for each attack category—Integrity, Privacy, Communication, and Aggregator/Robustness—emphasizing their underlying principles, current limitations, and research gaps.

\subsection{Integrity: Data and Model Poisoning, Backdoor Attacks}
\paragraph{Foundations.}
Early Byzantine-robust methods focus on filtering anomalous client updates before aggregation. \textit{Krum} and \textit{Multi-Krum} select updates closest to the majority in Euclidean space, tolerating up to a fixed fraction of malicious clients \cite{blanchard2017machine}. \textit{Trimmed Mean} and \textit{Median} aggregation perform coordinate-wise pruning of extremes, while \textit{Bulyan} improves stability by combining both selection and trimming phases \cite{yin2018byzantine, elmhamdi2018hidden}. These methods remain core baselines for integrity, but they assume i.i.d. data and synchronous participation—assumptions rarely valid in modern FL deployments.

\paragraph{Robust aggregation: }
Recent approaches adapt robustness to non-i.i.d. and large-scale FL. \textbf{FLTrust} \cite{cao2021fltrust} introduces a trusted reference dataset on the server, computes a reference update, and scores each client update by cosine similarity to this reference. Updates are normalized to prevent large outliers, then weighted by their trust scores. FLTrust achieves strong resilience against data and model poisoning—even with over 40\% malicious clients—but depends on server-side clean data. Complementary work introduces adaptive robust aggregation (e.g., momentum-based filtering or clustering-driven aggregation) that identifies groups of coherent updates rather than outlier removal alone.

In parallel, efficient statistical defenses such as the \textit{Coordinate-wise Median of Means} and \textit{Geometric Median} reduce computational overhead while preserving bounded-influence properties. Recent quasi-linear-time robust mean estimators 
% \cite{lee2025byzantine}
further close the gap between robustness and scalability.

\paragraph{Anomaly and outlier detection.}
Beyond rule-based aggregation, anomaly detection employs machine learning itself. Autoencoder-based detection and spectral clustering approaches learn the manifold of normal updates and isolate deviations. Some frameworks (e.g., AlignIns, Xu et al., 2025) analyze directional alignment of gradient vectors to capture stealthy manipulations that mimic benign norms but differ in orientation. Dual-layer filtering 
% \cite{ding2025defense} 
first partitions updates by local density (trust vs. suspicious groups), then re-evaluates residuals in each group, improving recall without sacrificing fairness to heterogeneous clients. These detection systems, however, require careful threshold tuning and historical context to distinguish diversity from malicious deviation.

\paragraph{Backdoor-specific defenses.}
Backdoor attacks \cite{bagdasaryan2020backdoor, xie2020dba, wang2021attack} exploit the fact that the global model generalizes triggers seen by only a few clients. SOTA defenses combine several complementary strategies:
\begin{itemize}
  \item \textbf{Norm clipping:} limits each update’s $\ell_2$ norm, mitigating single-round model replacement. Adaptive attackers may, however, scale updates just below clipping thresholds.
  \item \textbf{Directional filtering:} compares each update’s cosine similarity with both the global and majority directions to suppress deviating gradients.
  \item \textbf{Server-side audits:} evaluate updated models on a clean validation set or known canary triggers to detect accuracy drops or suspicious activations.
  \item \textbf{Model repair:} post-hoc pruning or fine-tuning on clean data, removing neurons strongly correlated with triggers (akin to Neural Cleanse).
\end{itemize}
DBA-style distributed triggers remain challenging because each adversarial client introduces only a fragment of the backdoor. Detecting these requires multi-round consistency checks or frequency analysis over update histories.

\paragraph{Trade-offs and insights.}
Robust aggregation ensures integrity but can degrade accuracy under data heterogeneity. Many algorithms assume bounded malicious fractions (often below 25–40\%) and synchronous updates. Anomaly detection improves flexibility but introduces computational overhead and potential false positives. Privacy measures like \textit{Secure Aggregation} conceal individual updates, making robust filtering harder—highlighting a persistent privacy–robustness tension.

\subsection{Privacy and Inference: Gradient Inversion, Membership, and Property Attacks}
\paragraph{Differential Privacy (DP).}
Differential privacy has become the dominant defense paradigm for inference risks. FL implementations typically use \textit{DP-FedAvg} or \textit{DP-FedSGD}, where each client clips gradients and adds Gaussian noise before sending updates. The added noise guarantees $(\varepsilon, \delta)$-DP, bounding how much any individual’s data can influence the global model \cite{zhu2019deep, geiping2020inverting, nasr2019comprehensive, melis2019exploiting}. Modern DP research improves the privacy–utility balance via:
\begin{itemize}
  \item Adaptive noise scheduling (less noise in later rounds as gradients stabilize),
  \item Layer-wise noise scaling—allocating privacy budgets per model layer,
  \item Advanced privacy accounting (e.g., Rényi DP or zCDP) for tighter bounds.
\end{itemize}
While DP thwarts direct gradient inversion, it can slow convergence and disproportionately impact small or non-i.i.d. clients.

\paragraph{Cryptographic defenses.}
\textbf{Secure Aggregation} (\textit{SecAgg}) \cite{bonawitz2017practical} ensures the server learns only the aggregated sum of client updates using additively homomorphic encryption and secret sharing. Even a curious server cannot inspect individual gradients, significantly reducing gradient leakage risk. SOTA variants (e.g., SecAgg+) enhance dropout tolerance and reduce bandwidth overhead for thousands of clients. Homomorphic encryption (HE) and secure multi-party computation (MPC) protocols extend this idea—allowing operations directly on encrypted updates—but remain resource-heavy. Systems such as FedML-HE \cite{liu2021fedmlhe} and hybrid approaches (HE + DP) balance confidentiality with performance, albeit at added complexity.

\paragraph{Gradient obfuscation and compression.}
Practical obfuscation (e.g., sparsification, quantization, or random rotation of gradients) limits information content. Although not providing formal DP guarantees, such schemes complicate inversion attacks and improve communication efficiency. Combined with DP or SecAgg, they provide layered protection with minor utility cost.

\paragraph{Trade-offs.}
DP offers formal privacy but at a measurable cost to accuracy. Strong noise (small $\varepsilon$) hinders utility, while weak noise leaves leakage. SecAgg adds communication rounds and depends on synchrony among clients. Combining DP and cryptography—common in production FL—offers strong guarantees but increases latency and server complexity. Importantly, privacy defenses often reduce visibility for anomaly or Sybil detection, requiring new privacy-compatible audit mechanisms.

\subsection{Communication and Sybil Defenses}
\paragraph{MITM and transport security.}
Communication-layer attacks are addressed with standard cryptographic protocols. FL frameworks (e.g., TensorFlow Federated, PySyft) integrate TLS/SSL encryption and mutual authentication. Model updates can be digitally signed or accompanied by message authentication codes to ensure integrity. While such measures prevent classic man-in-the-middle tampering, they do not protect against compromised endpoints or malicious clients—highlighting the need for higher-layer defenses \cite{lyu2020threats}.

\paragraph{Sybil detection and mitigation.}
Sybil attacks—where an adversary spawns multiple fake clients—amplify poisoning power. Algorithmic defenses such as \textbf{FoolsGold} \cite{fung2018mitigating} detect Sybils via update similarity: clients exhibiting consistently high cosine similarity across rounds receive reduced learning rates. FoolsGold adapts weighting dynamically, suppressing clusters of correlated updates without requiring explicit identification. Follow-up works explore:
\begin{itemize}
  \item Clustering-based defenses that identify communities of similar updates;
  \item Reputation and staking mechanisms penalizing unhelpful or redundant clients;
  \item Randomized client sampling per round to probabilistically limit Sybil influence.
\end{itemize}
Nonetheless, adaptive Sybils can introduce noise to decorrelate gradients, evading similarity checks. Moreover, highly correlated honest clients (e.g., from similar data distributions) may be unfairly down-weighted, underscoring fairness–security trade-offs.

\subsection{Aggregator and Robustness: Model Replacement, Free-Riding, Adversarial Examples}
\paragraph{Model replacement.}
To prevent malicious model substitution \cite{bagdasaryan2020backdoor, wang2021attack}, servers enforce per-update clipping, employ robust aggregation (median, Krum, FLTrust), and maintain validation-based audits to detect abrupt shifts in accuracy or decision boundaries. Advanced strategies compute the cosine divergence between consecutive global models or employ checksum-based verification in distributed aggregation (e.g., blockchain consensus). While effective, such redundancy increases training latency and requires careful fault tolerance.

\paragraph{Free-rider detection.}
Free-riders exploit FL by submitting trivial or fabricated updates. Techniques like FRIDA (Recasens et al., 2024) apply inference-based audits: if a client’s claimed dataset leaves no detectable trace in the updated model (e.g., via membership inference), the client is flagged as non-contributing. Simpler heuristics—tracking gradient norms, parameter change frequency, or local validation accuracy—also provide signals. However, DP or SecAgg can obscure such metrics, necessitating privacy-preserving contribution scoring or cryptographic proofs of work.

\paragraph{Adversarial robustness.}
Adversarial examples threaten inference integrity. \textbf{Federated Adversarial Training (FAT)} distributes adversarial training to clients: each generates perturbed samples (e.g., FGSM or PGD attacks) and optimizes local robustness objectives. Calibrated FAT (CalFAT) \cite{chen2022calfat} addresses label skew by aligning local logits, ensuring stable aggregation across diverse distributions. Empirically, FAT improves robustness to $L_\infty$-bounded perturbations but at substantial computational cost and reduced clean accuracy. Hybrid methods combine centralized adversarial fine-tuning with federated updates to balance cost and robustness.

\paragraph{Trade-offs.}
Defenses at the aggregator layer balance correctness, scalability, and transparency. Strong clipping and aggregation slow convergence; decentralized validation or blockchain consensus adds latency. FAT improves robustness but increases local computation and bandwidth. Integrating these defenses under constrained devices remains a core engineering challenge.


\newpage

\section*{Conclusion}
\hspace{1.3em} Federated Learning represents a paradigm shift in machine learning—one that enables large-scale collaboration without centralizing sensitive data. Yet, this decentralization also expands the attack surface, exposing new vulnerabilities across data, model, communication, and aggregation layers. This state-of-the-art survey has demonstrated that while FL offers strong privacy-by-design principles, its inherent openness requires a continuous, multi-layered defense strategy to ensure trustworthiness and operational resilience.

Robust aggregation mechanisms such as Krum, Bulyan, and FLTrust have matured to defend against poisoning and backdoor attacks, while privacy-preserving frameworks based on Differential Privacy and Secure Aggregation address gradient leakage and inference threats. Cryptographic protocols, anomaly detection, and adversarial training extend this protection to cover communication integrity and robustness against adaptive adversaries. However, each defense introduces trade-offs between privacy, accuracy, scalability, and interpretability, underscoring the need for holistic system design.

Traditional machine learning pipelines must evolve into secure, self-monitoring ecosystems capable of anticipating, detecting, and mitigating attacks in real time. Future MLSecOps pipelines for FL will integrate continuous monitoring, automated patching, and dynamic defense orchestration while maintaining compliance with data governance regulations.

Ultimately, the path forward lies in harmonizing security, privacy, and performance through layered, interoperable defenses. By embedding security as a priority throughout the FL lifecycle—from client selection to model deployment—Federated Learning can achieve both the scalability demanded by modern AI systems and the resilience required for real-world, adversarial environments.

\newpage

\bibliographystyle{IEEEtran}
\bibliography{references}





\end{document}