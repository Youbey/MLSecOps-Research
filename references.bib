
@article{bagdasaryan2020backdoor,
  title={Backdoor attacks on federated learning},
  author={Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  journal={Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2020}
}

@article{fang2020local,
  title={Local Model Poisoning Attacks to Byzantine-Robust Federated Learning},
  author={Fang, Meng and Cao, Xiaofei and Jia, Jiawei and Gong, Neil Zhenqiang},
  journal={USENIX Security Symposium},
  year={2020}
}

@article{blanchard2017machine,
  title={Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent},
  author={Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2017}
}

@article{bhagoji2019analyzing,
  title={Analyzing Federated Learning through an Adversarial Lens},
  author={Bhagoji, Arjun Nitin and Chakraborty, Supriyo and Mittal, Prateek and Calo, Seraphin},
  journal={Proceedings of the 36th International Conference on Machine Learning (ICML)},
  year={2019}
}

@article{baruch2019little,
  title={A Little is Enough: Circumventing Defenses for Distributed Learning},
  author={Baruch, Gilad and Baruch, Moran and Goldberg, Yoav},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{nasr2019comprehensive,
  title={Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning},
  author={Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  journal={IEEE Symposium on Security and Privacy (S\&P)},
  year={2019}
}

@article{melis2019exploiting,
  title={Exploiting Unintended Feature Leakage in Collaborative Learning},
  author={Melis, Luca and Song, Congzheng and Cristofaro, Emiliano De and Shmatikov, Vitaly},
  journal={IEEE Symposium on Security and Privacy (S\&P)},
  year={2019}
}

@article{zhu2019deep,
  title={Deep Leakage from Gradients},
  author={Zhu, Ligeng and Liu, Zhijian and Han, Song},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{geiping2020inverting,
  title={Inverting Gradients - How easy is it to break privacy in federated learning?},
  author={Geiping, Jonas and Bauermeister, Hartmut and Dr√∂ge, Heiko and Moeller, Michael},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@article{lyu2020threats,
  title={Threats to Federated Learning: A Survey},
  author={Lyu, Lingjuan and Yu, Han and Yang, Qiang},
  journal={ACM Computing Surveys},
  year={2020}
}

@article{fung2018mitigating,
  title={Mitigating Sybils in Federated Learning Poisoning},
  author={Fung, Clement and Yoon, Chris J and Beschastnikh, Ivan},
  journal={Proceedings of the 21st International Symposium on Research in Attacks, Intrusions and Defenses (RAID)},
  year={2018}
}

@article{wang2021attack,
  title={Attack of the Tails: Yes, You Really Can Backdoor Federated Learning},
  author={Wang, Yu and Hu, Yuan and Li, Guoming and Lin, Zhiqiang},
  journal={USENIX Security Symposium},
  year={2021}
}
